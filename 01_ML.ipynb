{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eed65037",
   "metadata": {},
   "source": [
    "## Holiday Package Prediction\n",
    "\n",
    "Author: **Ali Raza** \\\n",
    "Project Type: **Mock Project** \n",
    "\n",
    "\n",
    "## 1- Problem Statement\n",
    "Trips and Travel.com aims to optimize its marketing strategy by predicting whether customers are likely to purchase a newly launched holiday package. Using historical data on previous package purchases, the objective is to build a binary classification model that can forecast customer interest. This prediction will help the company decide whether launching a new package is likely to be successful, enabling better resource allocation, reduced marketing costs, and increased conversion rates.\n",
    "\n",
    "## 2- Data Collection\n",
    "Dataset is available on kaggle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63257cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Travel.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6717f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea048e04",
   "metadata": {},
   "source": [
    "## 3- Data Cleaning\n",
    "\n",
    "### Handling Missing Values\n",
    "\n",
    "1. Handling Missing Values\n",
    "2. Handling Duplicates\n",
    "3. Check Data Types\n",
    "4. Understand the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e6f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Checking missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fdaa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking all the categorical features to understand wether the\n",
    "## data containes any mistakes or not\n",
    "print(df['Gender'].value_counts())\n",
    "print(df['MaritalStatus'].value_counts())\n",
    "\n",
    "## perform the same operation on other categories as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb6d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender'] = df['Gender'].replace('Fe Male', 'Female')\n",
    "df['MaritalStatus'] = df['MaritalStatus'].replace('Single', 'Unmarried')\n",
    "\n",
    "df['Gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MaritalStatus'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20e3207",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe3bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking the Missing values\n",
    "## these are the Features with nan values\n",
    "\n",
    "## [expression for item in iterable if condition]\n",
    "\n",
    "features_with_na = [features for features in df.columns if df[features].isnull().sum()>=1]\n",
    "\n",
    "for i in features_with_na:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1223baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features_with_na:\n",
    "    print(feature, np.round(df[feature].isnull().mean()*100, 2), '% missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e7be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic stats on numerical columns\n",
    "\n",
    "df[features_with_na].select_dtypes(exclude='object').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2881b5",
   "metadata": {},
   "source": [
    "### Imputing Null values\n",
    "\n",
    "1. Impute Median value for Age column\n",
    "2. Impute Mode for Type of Contact\n",
    "3. Impute Median for Duration of Pitch\n",
    "4. Impute Mode for Number of Followups as it is discrete feature\n",
    "5. Impute Mode for Preffered Property Star\n",
    "6. Impute Median for Number of trips\n",
    "7. Impute Mode for Number of Children Visiting\n",
    "8. Impute Median for Monthly Income "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age.fillna(df.Age.median(), inplace=True)\n",
    "\n",
    "print(df.Age.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d88933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.TypeofContact.mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d30f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.TypeofContact.fillna(df.TypeofContact.mode()[0], inplace= True)\n",
    "print(df.TypeofContact.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df75846",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repeating the same procedure for all the Nan containing features\n",
    "\n",
    "df.DurationOfPitch.fillna(df.DurationOfPitch.median(), inplace = True)\n",
    "\n",
    "df.NumberOfFollowups.fillna(df.NumberOfFollowups.mode()[0], inplace=True)\n",
    "\n",
    "df.PreferredPropertyStar.fillna(df.PreferredPropertyStar.mode()[0], inplace=True)\n",
    "\n",
    "df.NumberOfTrips.fillna(df.NumberOfTrips.median(), inplace=True)\n",
    "\n",
    "df.NumberOfChildrenVisiting.fillna(df.NumberOfChildrenVisiting.mode()[0], inplace=True)\n",
    "\n",
    "df.MonthlyIncome.fillna(df.MonthlyIncome.median(), inplace=True)\n",
    "\n",
    "\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff75065",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c9dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('CustomerID', inplace=True, axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba50d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating new columns and removing unncessary columns\n",
    "\n",
    "df['TotalVisiting'] = df['NumberOfPersonVisiting'] + df['NumberOfChildrenVisiting']\n",
    "\n",
    "df.drop(columns= ['NumberOfPersonVisiting','NumberOfChildrenVisiting'], inplace=True, axis= 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ad962",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting all the Numeric features\n",
    "\n",
    "num_features = [feature for feature in df.columns if df[feature].dtype != 'O']\n",
    "\n",
    "## getting all the Categorical features\n",
    "\n",
    "cat_features = [feature for feature in df.columns if df[feature].dtype == 'O']\n",
    "\n",
    "print('Number of numeric features', len(num_features))\n",
    "print('Number of Categorical Features', len(cat_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce5bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Discrete Features \n",
    "\n",
    "discrete_features = [feature for feature in num_features if len(df[feature].unique())<=25]\n",
    "\n",
    "## Continuous Features\n",
    "continuous_features = [feature for feature in num_features if len(df[feature].unique())>25]\n",
    "\n",
    "print('Number of Discrete features', len(discrete_features))\n",
    "print('Number of Continuous features', len(continuous_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f77976",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6317dcdb",
   "metadata": {},
   "source": [
    "## 4- Train Test Split and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f95396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(['ProdTaken'], axis=1)\n",
    "y = df['ProdTaken']\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed31e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf31114",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6d29d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1548f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Column Transformer\n",
    "\n",
    "cat_features = X.select_dtypes(include='object').columns\n",
    "num_features = X.select_dtypes(exclude='object').columns\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "oh_transformer = OneHotEncoder(drop='first')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"OneHotEncoder\", oh_transformer, cat_features),\n",
    "        (\"StandardScaler\", numeric_transformer, num_features)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0172287",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69402e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying transformation in training dataset ----> fit_transform\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2817dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply transformation in test data using ----> transform\n",
    "\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb6f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bdab1d",
   "metadata": {},
   "source": [
    "## 5- Random Forest Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6744ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay, \\\n",
    "precision_score, recall_score, f1_score, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7775f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier()\n",
    "}\n",
    "\n",
    "for i in range(len(list(models))):\n",
    "    model = list(models.values())[i]\n",
    "    model.fit(X_train, y_train) ## Model training\n",
    "\n",
    "    ## Making Predictions\n",
    "    y_train_pred  = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    ## Training set performance \n",
    "    model_train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    model_train_f1 = f1_score(y_train, y_train_pred, average= 'weighted')\n",
    "    model_train_precision = precision_score(y_train, y_train_pred)\n",
    "    model_train_recall = recall_score(y_train, y_train_pred)\n",
    "    model_train_rocauc_score = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "    ## Test Performance\n",
    "    model_test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    model_test_f1 = f1_score(y_test, y_test_pred, average= 'weighted')\n",
    "    model_test_precision = precision_score(y_test, y_test_pred)\n",
    "    model_test_recall = recall_score(y_test, y_test_pred)\n",
    "    model_test_rocauc_score = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "    print(list(models.keys())[i])\n",
    "\n",
    "    print('Model Performance for Training Set')\n",
    "    print(\"- Accuracy: {:.4f}\".format(model_train_accuracy))\n",
    "    print(\"- F1 score: {:.4f}\".format(model_train_f1))\n",
    "    print(\"- Precision: {:.4f}\".format(model_train_precision))\n",
    "    print(\"- Recall: {:.4f}\".format(model_train_recall))\n",
    "    print(\"- ROC AUC Score: {:.4f}\".format(model_train_rocauc_score))\n",
    "\n",
    "    print('-'*35)\n",
    "\n",
    "    print(\"Model Performance for Test set\")\n",
    "    print(\"- Accuracy: {:.4f}\".format(model_test_accuracy))\n",
    "    print(\"- F1 score: {:.4f}\".format(model_test_f1))\n",
    "    print(\"- Precision: {:.4f}\".format(model_test_precision))\n",
    "    print(\"- Recall: {:.4f}\".format(model_test_recall))\n",
    "    print(\"- ROC AUC Score: {:.4f}\".format(model_test_rocauc_score))\n",
    "\n",
    "    print('='*35)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "rf_params = {\n",
    "    \"max_depth\": [5,8,15,None,10],\n",
    "    \"max_features\": [5,7,\"auto\",8],\n",
    "    \"min_samples_split\": [2,8,15,20],\n",
    "    \"n_estimators\": [100,200,500,1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Models list for hyperparameter tuning\n",
    "randomcv_models = [\n",
    "    (\"RF\", RandomForestClassifier(), rf_params)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86d72d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "model_param = {}\n",
    "\n",
    "for name,model,params in randomcv_models:\n",
    "    random = RandomizedSearchCV(estimator=model,\n",
    "                                param_distributions=params,\n",
    "                                n_iter=100,\n",
    "                                cv=3,\n",
    "                                verbose=2,\n",
    "                                n_jobs=-1)\n",
    "    \n",
    "    random.fit(X_train, y_train)\n",
    "    model_param[name] = random.best_params_\n",
    "\n",
    "\n",
    "for model_name in model_param:\n",
    "    print(f'---------------- Best Params for {model_name} ----------------')\n",
    "    print(model_param[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d643cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=500, min_samples_split=2,\n",
    "                                            max_features=8, max_depth=None)\n",
    "}\n",
    "\n",
    "for i in range(len(list(models))):\n",
    "    model = list(models.values())[i]\n",
    "    model.fit(X_train, y_train) ## Model training\n",
    "\n",
    "    ## Making Predictions\n",
    "    y_train_pred  = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    ## Training set performance \n",
    "    model_train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    model_train_f1 = f1_score(y_train, y_train_pred, average= 'weighted')\n",
    "    model_train_precision = precision_score(y_train, y_train_pred)\n",
    "    model_train_recall = recall_score(y_train, y_train_pred)\n",
    "    model_train_rocauc_score = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "    ## Test Performance\n",
    "    model_test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    model_test_f1 = f1_score(y_test, y_test_pred, average= 'weighted')\n",
    "    model_test_precision = precision_score(y_test, y_test_pred)\n",
    "    model_test_recall = recall_score(y_test, y_test_pred)\n",
    "    model_test_rocauc_score = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "    print(list(models.keys())[i])\n",
    "\n",
    "    print('Model Performance for Training Set')\n",
    "    print(\"- Accuracy: {:.4f}\".format(model_train_accuracy))\n",
    "    print(\"- F1 score: {:.4f}\".format(model_train_f1))\n",
    "    print(\"- Precision: {:.4f}\".format(model_train_precision))\n",
    "    print(\"- Recall: {:.4f}\".format(model_train_recall))\n",
    "    print(\"- ROC AUC Score: {:.4f}\".format(model_train_rocauc_score))\n",
    "\n",
    "    print('-'*35)\n",
    "\n",
    "    print(\"Model Performance for Test set\")\n",
    "    print(\"- Accuracy: {:.4f}\".format(model_test_accuracy))\n",
    "    print(\"- F1 score: {:.4f}\".format(model_test_f1))\n",
    "    print(\"- Precision: {:.4f}\".format(model_test_precision))\n",
    "    print(\"- Recall: {:.4f}\".format(model_test_recall))\n",
    "    print(\"- ROC AUC Score: {:.4f}\".format(model_test_rocauc_score))\n",
    "\n",
    "    print('='*35)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the ROC AUC Curve\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "## ADD the models to the list that you want to view on the ROC plot\n",
    "\n",
    "auc_models = [\n",
    "    {\n",
    "        'label': 'Random Forest Classifier',\n",
    "        'model' : RandomForestClassifier(n_estimators=500, min_samples_split=2,\n",
    "                                            max_features=8, max_depth=None),\n",
    "        'auc': 0.8398\n",
    "\n",
    "\n",
    "    }\n",
    "]\n",
    "\n",
    "## create loop through all models\n",
    "\n",
    "for algo in auc_models:\n",
    "    model = algo['model'] ## select the model\n",
    "    model.fit(X_train, y_train) ## train the model\n",
    "    ## Compute false positive rate and true positive rate\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, model.predict_prob(X_test)[:,1])\n",
    "    ## Calculate area under the curve to display on the plot\n",
    "\n",
    "    plt.plot(fpr, tpr,label= '% ROC (area = %0.2f)' % (algo['label']))\n",
    "\n",
    "\n",
    "\n",
    "## Custom settings for the plot\n",
    "\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Specificity(False Positive Rate)')\n",
    "plt.ylabel('Sensitivity(True Positive Rate)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"auc.png\")\n",
    "plt.show()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a70f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
